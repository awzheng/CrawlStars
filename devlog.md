# CrawlStars DevLog

This is an authentic and energetic devlog about why and how I built CrawlStars. I will justify all of my design choices by sharing my thought processes for every decision I make. I would appreciate any feedback you may have!

Instagram, Discord @awzheng

## Why?

After playing around with Zen Browser over the 1A/1B winter break I immediately realized that the URL bar autocomplete was awful. I decided to take what I knew about SEO (from years of DECA) and attempted to build my own search engine while learning Go and MongoDB in the process. I've gained so much understanding and respect towards the search engine industry (and would love to join them soon...)

## Episode 1: Starting Out

### Design Choices

#### Why Go over Python?

- Go is compiled. It's faster and more efficient than Python. However, the dopamine loss compared to my days of seeing my frontend update immediately when coding in Python/JS was a culture shock. lol.
- Goroutines let me spawn multiple concurrent workers to crawl pages at the same time. Having 10 workers made me feel like a 10x developer while only taking up 10% of my CPU and several MBs of storage up in MongoDB Atlas (more on that later).
- Being statically typed means it is less prone to errors. Yes it's important since lots of websites (such as Reddit) tend to block crawlers and cause 403 errors.

#### Why MongoDB Atlas over SQL?

The scaffolding for CrawlStars was initially built using PostgreSQL, but it was too rigid and error-prone for my purposes. Instead I pivoted to MongoDB Atlas, a NoSQL database. But why?

1. JSON is flexible, and my crawler extracts data that looks like json anyway, thus storing it in a NoSQL database is a natural fit.
2. MongoDB is optimized for high-speed ingestion which is advantageous for CrawlStars which uses 10 concurrent workers to dump data fast.
3. Atlas Search is OP. A fuzzy search engine with relevance ratings that I can scale from 0-5 stars? Built into the database??? Yes please!

## Project Diagrams

### Write Path
![CrawlStars Write Path](assets/crawlstars-write-path.png)

### Read Path
![CrawlStars Read Path](assets/crawlstars-read-path.png)
### Project Structure

```
CrawlStars/
├── cmd/
│   ├── crawler/main.go    # Crawler entry point
│   └── server/main.go     # Search API server
├── internal/
│   ├── crawler/
│   │   └── crawler.go     # Concurrent crawler logic
│   └── database/
│       └── mongo.go       # MongoDB operations and search
├── web/
│   └── index.html         # Search interface
└── go.mod
```

Before I jump into the design choices of each file, let's take a holistic look at the project structure.

| Directory/File | Description |
|----------------|-------------|
| **cmd/** | Contains the main entry points for the crawler and server |
| `cmd/crawler/main.go` | Entry point for the crawler |
| `cmd/server/main.go` | Entry point for the search API server |
| **internal/** | Contains the internal logic for the crawler and database |
| `internal/crawler/crawler.go` | Concurrent crawler logic containing all member functions|
| `internal/database/mongo.go` | Takes care of MongoDB operations, UI through the website/terminal, and search functions |
| **web/** | Contains the frontend code for the search interface |
| `web/index.html` | Search interface |
| **go.mod** | Contains the dependencies for the project |
| **go.sum** | Contains the checksums for the dependencies. Automatically generated by `go mod tidy`. Security layer that ensures our dependencies haven't been modified (maliciously/accidentally) |




